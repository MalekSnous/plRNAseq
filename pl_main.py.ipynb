{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install --user --upgrade pip\n",
    "# !python3 -m venv plenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!source plenv/bin/activate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import argparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import Memory\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "#from pl_model import pl_SVM, pl_nn_RIC, pl_KNN, pl_ric_light, pl_ultra_light_ric\n",
    "from load_pl_data import load_dataset_partial_label\n",
    "from pl_model import pl_hKNN, plSVM, pl_nn_prototybe_based\n",
    "\n",
    "#from pl_setting import split_partial_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developpement = False  # debugging\n",
    "device = 'cpu'  #device = 'cuda:0'\n",
    "PATH = os.getcwd()\n",
    "path_file_abs = PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_liste = ['Packer', 'Paul', 'Planaria', 'linear', 'half', 'binary']\n",
    "dataset = dataset_liste[1]\n",
    "\n",
    "# PARTIAL LABELLING SETTING\n",
    "overlap = 0\n",
    "p = 1.0\n",
    "k = 4\n",
    "I = 'I0'\n",
    "\n",
    "# METHOD\n",
    "method_liste = ['PB', 'plSVM', 'pl_hkNN']\n",
    "method = method_liste[1]\n",
    "linear = True  #False #neural network for PB or kernel for SVM\n",
    "indice_network = 2 if linear == False else 0\n",
    "\n",
    "#HIERARCHY\n",
    "\n",
    "choix_C_liste = ['flat', 'C']\n",
    "choix_C = choix_C_liste[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset in ['linear', 'half', 'binary']:\n",
    "    alpha = 0.1  #0.5\n",
    "    programs = 50\n",
    "    lenght_tree = 8\n",
    "    topology = 'half'\n",
    "    nfactor = 5\n",
    "    p = 200\n",
    "\n",
    "    name_tree = '_' + str(programs) + '_' + str(alpha) + '_nfactor_' + str(nfactor)\n",
    "    path_file = path_file_abs + 'data/datasets/Tree/' + str(topology) + '/' + str(lenght_tree) + '/'\n",
    "    dataset = str(topology) + '_' + str(lenght_tree) + '_' + str(programs) + '_' + str(alpha)\n",
    "\n",
    "    X = np.load(path_file + 'X_Tree' + str(name_tree) + '.npy', allow_pickle=True)\n",
    "    y = np.load(path_file + 'y_Tree' + str(name_tree) + '.npy', allow_pickle=True).tolist()\n",
    "    y = [int(element) for element in y]\n",
    "    mat_dist = np.load(path_file + 'Tree' + str(name_tree) + '_mat_dist.npy')\n",
    "    time = np.load(path_file + 'Tree' + str(name_tree) + '_pseudotime.npy')\n",
    "\n",
    "    C = torch.tensor(mat_dist)\n",
    "    X = np.array(X, 'float')\n",
    "    c = C.shape[0]\n",
    "    y_id = torch.eye(c)\n",
    "    print(dataset)\n",
    "\n",
    "    print(name_tree, X.shape, len(y))\n",
    "\n",
    "if dataset in ['Packer', 'Paul', 'Planaria']:\n",
    "\n",
    "    if dataset == 'Packer':\n",
    "        path_file = PATH + '/data/datasets/' + str(dataset) + '/'\n",
    "        try:\n",
    "            X = np.load(path_file + 'X_pca.npy', allow_pickle=True)\n",
    "            y = np.load(path_file + 'y.npy', allow_pickle=True).tolist()\n",
    "            mat_dist = np.load(path_file + str(dataset) + '_mat_dist.npy')\n",
    "            print('X,y loaded')\n",
    "        except:\n",
    "\n",
    "            df = pd.read_csv(path_file + dataset + '.csv')\n",
    "            X = np.asarray(df)\n",
    "            X = X[:, :-1]\n",
    "            np.save(path_file + 'X', X)\n",
    "            try:\n",
    "                y_names = df['labels'].tolist()\n",
    "            except:\n",
    "                y_names = df['cell_type'].tolist()\n",
    "            names = np.load(path_file + dataset + '/' + dataset + '_names.npy').tolist()\n",
    "            names = sorted(list(set(names)))\n",
    "            y = [names.index(i) for i in y_names]\n",
    "            np.save(path_file + 'y', y)\n",
    "            mat_dist = np.load(path_file + str(dataset) + '_mat_dist.npy')\n",
    "\n",
    "    if dataset == 'Planaria' or dataset == 'Paul':\n",
    "        #\n",
    "        path_file = PATH + '/data/datasets/' + str(dataset) + '/'\n",
    "        X = np.load(path_file + 'sample/X.npy', allow_pickle=True)\n",
    "        y = np.load(path_file + 'sample/y.npy', allow_pickle=True).tolist()\n",
    "\n",
    "        mat_dist = np.load(path_file + str(dataset) + '_mat_dist.npy')\n",
    "        # except :\n",
    "        #     path_file = os.getcwd()+'/data/datasets/'+str(dataset)+'/'\n",
    "        #     X = np.load(path_file+'sample/X.npy', allow_pickle=True)\n",
    "        #     y= np.load(path_file+'sample/y.npy', allow_pickle=True).tolist()\n",
    "\n",
    "        #     mat_dist = np.load(path_file + str(dataset)+'_mat_dist.npy')\n",
    "\n",
    "        if dataset == 'Paul':\n",
    "            X, y = X[:-1], y[:-1]\n",
    "\n",
    "    C = torch.Tensor(mat_dist)\n",
    "    X = np.vstack(X).astype(np.float64)\n",
    "    print(X.shape, len(y))\n",
    "    #X= torch.FloatTensor(X)\n",
    "    c = C.shape[0]\n",
    "\n",
    "mat_C = C if choix_C == 'C' else torch.ones((c, c)) - torch.eye(c)\n",
    "C, c, X_train_s, X_train_ws, y_train_s, y_train_ws, \\\n",
    "    y_train_s_prior, y_train_ws_prior, X_test_s, X_test_ws, \\\n",
    "    y_test_s, y_test_ws, y_test_s_prior, y_test_ws_prior \\\n",
    "    = load_dataset_partial_label(PATH=PATH,\n",
    "                                 dataset=dataset,\n",
    "                                 overlap=overlap,\n",
    "                                 I=I,\n",
    "                                 t=0,\n",
    "                                 sub_proportion=p,\n",
    "                                 k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dict_0(method):\n",
    "    if method == 'plSVM':\n",
    "        epochs = 5 if developpement else 101\n",
    "        kernel = True if indice_network == 2 else False\n",
    "        dict_0 = {'W': nn.Sequential(nn.Linear(X_train_s.size(1), c)),\n",
    "                  'optimizer': 'Adam',\n",
    "                  'lr_P': 1.e-5,\n",
    "                  'lambdaa': 1.e-2,\n",
    "                  'epochs': epochs,\n",
    "                  'device': device,\n",
    "                  'C': mat_C,\n",
    "                  'C_score': C,\n",
    "                  'kernel': kernel}\n",
    "        tiny_model = plSVM(W=nn.Sequential(nn.Linear(X_train_s.size(1), c)), )\n",
    "\n",
    "    if method == 'plhKNN':\n",
    "        dict_0 = {'C': mat_C, 'C_score': C}\n",
    "        tiny_model = pl_hKNN()\n",
    "\n",
    "    if method == 'PB':\n",
    "        epochs_regression, epoch_xsi = 101, 101\n",
    "        if developpement:\n",
    "            epochs_regression, epoch_xsi = 5, 5\n",
    "        g = X_train_s.shape[1]\n",
    "        liste_nn = [\n",
    "            nn.Sequential(nn.Linear(g, c)),\n",
    "            nn.Sequential(nn.Linear(g, c), nn.Tanh(), nn.Linear(c, c)),\n",
    "            nn.Sequential(nn.Linear(g, c), nn.Tanh(), nn.Linear(c, c), nn.Tanh(), nn.Linear(c, c)),\n",
    "            nn.Sequential(nn.Linear(g, c), nn.Tanh(), nn.Linear(c, c), nn.Tanh(), nn.Linear(c, c), nn.Tanh(),\n",
    "                          nn.Linear(c, c)),\n",
    "        ]\n",
    "        P = liste_nn[indice_network]\n",
    "\n",
    "        dict_0 = {'Network': P,\n",
    "                  'optimizer': 'Adam',\n",
    "                  'lr_P': 1.e-5,\n",
    "                  'lambdaa': 1.e-2,\n",
    "                  'lambdaa_solution_regression': 1.e-2,\n",
    "                  'epochs_regression': epochs_regression,\n",
    "                  'epochs_xsi': epoch_xsi,\n",
    "                  'device': device,\n",
    "                  'C': mat_C,\n",
    "                  'C_score': C,\n",
    "                  'distance': 'correlation'}\n",
    "        tiny_model = pl_nn_prototybe_based()\n",
    "    return dict_0, tiny_model\n",
    "\n",
    "\n",
    "def init_method(method, dict_entry):\n",
    "    if method == 'plSVM':\n",
    "        model = plSVM(W=nn.Sequential(nn.Linear(X_train_s.size(1), c)),\n",
    "                      C=dict_entry['C'],\n",
    "                      C_score=dict_entry['C_score'],\n",
    "                      device=dict_entry['device'],\n",
    "                      lambdaa=dict_entry['lambdaa'],\n",
    "                      epochs=dict_entry['epochs'],\n",
    "                      kernel=dict_entry['kernel'], )\n",
    "\n",
    "    if method == 'plhKNN':\n",
    "        model = pl_hKNN(\n",
    "            k=dict_entry['k'],\n",
    "            C=dict_entry['C'],\n",
    "            C_score=dict_entry['C_score']\n",
    "\n",
    "        )\n",
    "\n",
    "    if method == 'PB':\n",
    "        model = pl_nn_prototybe_based(Network=dict_entry['Network'],\n",
    "                                      lambdaa=dict_entry['lambdaa'],\n",
    "                                      lambdaa_solution_regression=dict_entry['lambdaa_solution_regression'],\n",
    "                                      optimizer=dict_entry['optimizer'],\n",
    "                                      distance=dict_entry['distance'],\n",
    "                                      lr_P=dict_entry['lr_P'],\n",
    "                                      epochs_regression=dict_entry['epochs_regression'],\n",
    "                                      epochs_xsi=dict_entry['epochs_xsi'],\n",
    "                                      device=dict_entry['device'],\n",
    "                                      C=dict_entry['C'],\n",
    "                                      C_score=dict_entry['C_score'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T23:12:44.989833134Z",
     "start_time": "2023-07-24T23:12:44.947274447Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_dict_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-59aeda598cd0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdic_0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minit_dict_0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minit_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdic_0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'plhKNN'\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mchoix_C\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'flat'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'init_dict_0' is not defined"
     ]
    }
   ],
   "source": [
    "dic_0 = init_dict_0(method)[0]\n",
    "model = init_method(method, dic_0)\n",
    "if method == 'plhKNN' and choix_C == 'flat':\n",
    "    model.flat = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FIT')\n",
    "model.fit(X_train=torch.cat((X_train_s, X_train_ws), dim=0),\n",
    "          y_train=[[yi] for yi in y_train_s] + y_train_ws_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PRED TEST')\n",
    "pred_train_s = model.predict(X=X_train_s, y_prior=None)\n",
    "pred_train_s_prior = model.predict(X=X_train_s, y_prior=y_train_s_prior)\n",
    "pred_train_ws = model.predict(X=X_train_ws, y_prior=None)\n",
    "pred_train_ws_prior_s = model.predict(X=X_train_ws, y_prior=y_train_ws_prior)\n",
    "\n",
    "pred_test_s = model.predict(X=X_test_s, y_prior=None)\n",
    "pred_test_s_prior = model.predict(X=X_test_s, y_prior=y_test_s_prior)\n",
    "pred_test_ws = model.predict(X=X_test_ws, y_prior=None)\n",
    "pred_test_ws_prior_s = model.predict(X=X_test_ws, y_prior=y_test_ws_prior)\n",
    "\n",
    "performance = [model.score(y_test=y_train_s, y_pred=pred_train_s),\n",
    "               model.score(y_test=y_train_s, y_pred=pred_train_s_prior),\n",
    "               model.score(y_test=y_train_ws, y_pred=pred_train_ws),\n",
    "               model.score(y_test=y_train_ws, y_pred=pred_train_ws_prior_s),\n",
    "               model.score(y_test=y_test_s, y_pred=pred_test_s),\n",
    "               model.score(y_test=y_test_s, y_pred=pred_test_s_prior),\n",
    "               model.score(y_test=y_test_ws, y_pred=pred_test_ws),\n",
    "               model.score(y_test=y_test_ws, y_pred=pred_test_ws_prior_s),\n",
    "\n",
    "               model.error(y_test=y_train_s, y_pred=pred_train_s, C=model.C_score),\n",
    "               model.error(y_test=y_train_s, y_pred=pred_train_s_prior, C=model.C_score),\n",
    "               model.error(y_test=y_train_ws, y_pred=pred_train_ws, C=model.C_score),\n",
    "               model.error(y_test=y_train_ws, y_pred=pred_train_ws_prior_s, C=model.C_score),\n",
    "               model.error(y_test=y_test_s, y_pred=pred_test_s, C=model.C_score),\n",
    "               model.error(y_test=y_test_s, y_pred=pred_test_s_prior, C=model.C_score),\n",
    "               model.error(y_test=y_test_ws, y_pred=pred_test_ws, C=model.C_score),\n",
    "               model.error(y_test=y_test_ws, y_pred=pred_test_ws_prior_s, C=model.C_score),\n",
    "\n",
    "               ]\n",
    "\n",
    "print('dataset :', dataset, ',overlap : ', overlap, ',n/p : ', p, ',k : ', k, ',I : ', I)\n",
    "print('Method : ', method, ' linear : ', linear)\n",
    "print('supervised test set accuracy : ', performance[4])\n",
    "print('partial label test set accuracy : ', performance[6])\n",
    "print('partial label test set accuracy with prior : ', performance[7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
